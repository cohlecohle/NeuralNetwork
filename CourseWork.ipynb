{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CourseWork.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqmbC0SEIrRG"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import shutil\n",
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "import tqdm\n",
        "import time\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "import albumentations\n",
        "from albumentations import pytorch as AT\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za2iGw7tZwa2"
      },
      "source": [
        "hyper_batch_size = 128\n",
        "hyper_num_workers = 10\n",
        "hyper_img_size = 256\n",
        "hyper_seed = 42\n",
        "hyper_training_patience = 5\n",
        "hyper_lr = 0.0005\n",
        "hyper_sheduler_factor = 0.2\n",
        "hyper_sheduler_patience = 2\n",
        "hyper_train_factor = 0.7\n",
        "hyper_val_test_factor = 0.5\n",
        "hyper_n_epochs = 50"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZSEr_9NTWdU"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEk5gwXHsfyZ"
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "seed_everything(hyper_seed)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wZSiCj6QNxd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctdmxz5tXRC9"
      },
      "source": [
        "with zipfile.ZipFile('/content/drive/MyDrive/Colab Notebooks/CourseWorkData/train.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjQasDxldc9n"
      },
      "source": [
        "train_dir = '/content/train'\n",
        "train_files = os.listdir(train_dir)\n",
        "print(len(train_files))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e505yM9qeZk-"
      },
      "source": [
        "train_files[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF5klQV3frJP"
      },
      "source": [
        "dataframe = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CourseWorkData/train_labels.csv', sep=',')\n",
        "dataframe[dataframe.columns.values[0]] = dataframe[dataframe.columns.values[0]].str.strip('./train/')\n",
        "print(dataframe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5suwRzYhJ-7"
      },
      "source": [
        "n_labels = dataframe[dataframe.columns.values[1]].nunique()\n",
        "print(n_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd3BOMIPQxw7"
      },
      "source": [
        "labels_raw = dataframe[dataframe.columns.values[1]].unique()\n",
        "labels_dict = dict(zip(labels_raw, list(range(n_labels))))\n",
        "print(labels_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xyi8N9JbaVo"
      },
      "source": [
        "def labels_to_index(labels):\n",
        "    indices = []\n",
        "    for l in labels:\n",
        "        indices.append(labels_dict[l])\n",
        "    return indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3bM_BeFckvp"
      },
      "source": [
        "indexed_labels = labels_to_index(dataframe[dataframe.columns.values[1]].values)\n",
        "print(len(indexed_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCVf08mxJdht"
      },
      "source": [
        "sklearn_train_values, sklearn_test_val_values, sklearn_train_labels, sklearn_test_val_labels = train_test_split(\n",
        "    dataframe.values,\n",
        "    indexed_labels,\n",
        "    test_size = 1 - hyper_train_factor,\n",
        "    random_state = hyper_seed,\n",
        "    stratify = indexed_labels\n",
        ")\n",
        "\n",
        "sklearn_test_values, sklearn_val_values, sklearn_test_labels, sklearn_val_labels = train_test_split(\n",
        "    sklearn_test_val_values,\n",
        "    sklearn_test_val_labels,\n",
        "    test_size = hyper_val_test_factor,\n",
        "    random_state = hyper_seed,\n",
        "    stratify = sklearn_test_val_labels\n",
        ")\n",
        "\n",
        "print(len(sklearn_train_values), len(sklearn_test_values), len(sklearn_val_values))\n",
        "print(len(sklearn_train_values) + len(sklearn_test_values) + len(sklearn_val_values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiEty9JIoPUQ"
      },
      "source": [
        "train_dataframe = pd.DataFrame(sklearn_train_values, columns = dataframe.columns.values)\n",
        "val_dataframe = pd.DataFrame(sklearn_val_values, columns = dataframe.columns.values)\n",
        "test_dataframe = pd.DataFrame(sklearn_test_values, columns = dataframe.columns.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDgEIBanWhlf"
      },
      "source": [
        "dataframe_series = dataframe.set_index(dataframe.columns.values[0]).squeeze()\n",
        "print(dataframe_series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOe-mmAqhW3k"
      },
      "source": [
        "train_labels = labels_to_index(train_dataframe[train_dataframe.columns.values[1]].values)\n",
        "class_sample_count = np.unique(train_labels, return_counts=True)[1]\n",
        "weight = 1. / class_sample_count\n",
        "samples_weight = np.array([weight[t] for t in train_labels])\n",
        "samples_weight = torch.from_numpy(samples_weight)\n",
        "print(samples_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmmFVYx0yPXu"
      },
      "source": [
        "sampler = WeightedRandomSampler(\n",
        "    weights = samples_weight,\n",
        "    num_samples = len(samples_weight),\n",
        "    replacement = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szRgFIYUsblJ"
      },
      "source": [
        "class SportsDataset(Dataset):\n",
        "    def __init__(self, file_list, dir, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.dir = dir\n",
        "        self.transform = transform\n",
        "\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image = cv2.imread(os.path.join(self.dir, self.file_list[idx]))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        self.label = labels_dict[dataframe_series[self.file_list[idx]]]\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image)\n",
        "            image = augmented['image']\n",
        "        \n",
        "        return image, self.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0MNtHg5Z8Ib"
      },
      "source": [
        "data_transforms = albumentations.Compose([\n",
        "    albumentations.Resize(hyper_img_size, hyper_img_size),\n",
        "    albumentations.HorizontalFlip(p=0.5),\n",
        "    albumentations.RandomBrightness(),\n",
        "    albumentations.ChannelShuffle(p=0.5),\n",
        "    albumentations.ShiftScaleRotate(rotate_limit=45, scale_limit=0.10),\n",
        "    albumentations.HueSaturationValue(),\n",
        "    albumentations.Normalize(),\n",
        "    AT.ToTensor()\n",
        "    ])\n",
        "\n",
        "data_transforms_test = albumentations.Compose([\n",
        "    albumentations.Resize(hyper_img_size, hyper_img_size),\n",
        "    albumentations.HorizontalFlip(),\n",
        "    albumentations.RandomRotate90(),\n",
        "    albumentations.Normalize(),\n",
        "    AT.ToTensor()\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHOSxJKBaJn-"
      },
      "source": [
        "train_set = SportsDataset(\n",
        "    file_list = train_dataframe[train_dataframe.columns.values[0]].values, \n",
        "    dir = train_dir, \n",
        "    transform = data_transforms\n",
        ")\n",
        "\n",
        "val_set = SportsDataset(\n",
        "    file_list = val_dataframe[val_dataframe.columns.values[0]].values,\n",
        "    dir = train_dir,\n",
        "    transform = data_transforms\n",
        ")\n",
        "\n",
        "test_set = SportsDataset(\n",
        "    file_list = test_dataframe[test_dataframe.columns.values[0]].values,\n",
        "    dir = train_dir,\n",
        "    transform = data_transforms_test\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOcrxD9HRdWJ"
      },
      "source": [
        "train_loader = DataLoader(\n",
        "    dataset = train_set,\n",
        "    batch_size = hyper_batch_size,\n",
        "    sampler = sampler,\n",
        "    shuffle = False,\n",
        "    pin_memory = True    \n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    dataset = val_set,\n",
        "    batch_size = hyper_batch_size,\n",
        "    shuffle = True,\n",
        "    pin_memory = True    \n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset = test_set,\n",
        "    batch_size = hyper_batch_size,\n",
        "    shuffle = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GckpIPzaR6UU"
      },
      "source": [
        "samples, targets = next(iter(train_loader))\n",
        "plt.figure(figsize=(16,24))\n",
        "grid_imgs = torchvision.utils.make_grid(samples[:24])\n",
        "np_grid_imgs = grid_imgs.numpy()\n",
        "print(targets[:24])\n",
        "plt.imshow(np.transpose(np_grid_imgs, (1,2,0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow2OXBOhT9U8"
      },
      "source": [
        "model = torchvision.models.resnet101(pretrained=True, progress=True)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "model.fc = nn.Linear(model.fc.in_features, n_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMJZnLpJWex9"
      },
      "source": [
        "def train_model(training_model, train_loader, valid_loader, criterion, optimizer, n_epochs):\n",
        "    training_model.to(device)\n",
        "    valid_loss_min = np.Inf\n",
        "    patience = 5\n",
        "    p = 0\n",
        "    stop = False\n",
        "    total_train_loss = []\n",
        "    total_val_loss = []\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        print(time.ctime(), 'Epoch:', epoch)\n",
        "        training_model.train()\n",
        "        train_loss = []\n",
        "        for batch_i, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device).long()\n",
        "            optimizer.zero_grad()\n",
        "            output = training_model(data)\n",
        "            loss = criterion(output, target)\n",
        "            train_loss.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "          \n",
        "        training_model.eval()\n",
        "        val_loss = []\n",
        "        for batch_i, (data, target) in enumerate(valid_loader):\n",
        "            data, target = data.to(device), target.to(device).long()\n",
        "            output = training_model(data)\n",
        "            loss = criterion(output, target)\n",
        "            val_loss.append(loss.item())\n",
        "\n",
        "        print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}.')\n",
        "\n",
        "        total_train_loss.append(np.mean(train_loss))\n",
        "        total_val_loss.append(np.mean(val_loss))\n",
        "\n",
        "        valid_loss = np.mean(val_loss)\n",
        "        scheduler.step(valid_loss)\n",
        "        if valid_loss <= valid_loss_min:\n",
        "            print(\n",
        "                'Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "                    valid_loss_min,\n",
        "                    valid_loss\n",
        "                )\n",
        "            )\n",
        "            torch.save(training_model.state_dict(), 'model.pt')\n",
        "            valid_loss_min = valid_loss\n",
        "            p = 0\n",
        "        \n",
        "        if valid_loss > valid_loss_min:\n",
        "            p += 1\n",
        "            print(f'{p} epochs of increasing val loss')\n",
        "            if p > patience:\n",
        "                print('Stopping training')\n",
        "                stop = True\n",
        "                break        \n",
        "\n",
        "        if stop:\n",
        "            break\n",
        "    return training_model, total_train_loss, total_val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb0h4mX_X93l"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=hyper_lr, amsgrad=True)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=hyper_sheduler_factor, patience=hyper_sheduler_patience)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHDuRTGFYsiG"
      },
      "source": [
        "trained_model, train_loss, val_loss = train_model(\n",
        "    training_model = model, \n",
        "    train_loader = train_loader, \n",
        "    valid_loader = valid_loader, \n",
        "    criterion = criterion, \n",
        "    optimizer = optimizer, \n",
        "    n_epochs = hyper_n_epochs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljrGm0XzcNk0"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ax1.plot(train_loss)\n",
        "ax2.plot(val_loss)\n",
        "ax1.set_ylabel(\"Train Loss\")\n",
        "ax1.set_xlabel(\"Epochs\")\n",
        "ax2.set_ylabel(\"Validaation Loss\")\n",
        "ax2.set_xlabel(\"Epochs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0ZcwGf5ZMiR"
      },
      "source": [
        "model.state_dict(torch.load('/content/model.pt'))\n",
        "print('Success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYQa8CGIZPMm"
      },
      "source": [
        "total_preds = 0\n",
        "correct_preds = 0\n",
        "model.to(device)\n",
        "model.eval()\n",
        "pred_list = []\n",
        "for sample, targets in test_loader:\n",
        "    with torch.no_grad():\n",
        "        sample = sample.to(device)\n",
        "        output = model(sample)\n",
        "        pred = torch.sigmoid(output)\n",
        "        pred = pred.cpu().argmax(dim=1)\n",
        "        pred_list = torch.empty(0)\n",
        "        pred_list = torch.cat((pred_list, pred))\n",
        "        total_preds += len(pred_list)\n",
        "        correct_preds += (targets == pred_list).sum()\n",
        "\n",
        "accuracy = 100.0 * correct_preds.float().item() / float(total_preds)\n",
        "print(f'Accuracy: {accuracy}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD5jmfeqamOO"
      },
      "source": [
        "sample, _ = next(iter(test_loader))\n",
        "sample = sample.to(device)\n",
        "model.to(device)\n",
        "output = model(sample)\n",
        "pred = torch.sigmoid(output)\n",
        "pred = pred.cpu().detach().numpy()\n",
        "print([p.argmax() for p in pred])\n",
        "plt.figure(figsize=(16,24))\n",
        "grid_imgs = torchvision.utils.make_grid(sample[:24])\n",
        "np_grid_imgs = grid_imgs.cpu().numpy()\n",
        "plt.imshow(np.transpose(np_grid_imgs, (1,2,0)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}